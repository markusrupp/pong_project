{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "## all packages\n",
        "! pip install gymnasium[all]\n",
        "!apt-get install git\n",
        "!git clone https://github.com/magni84/gym_RLcourse.git\n",
        "%cd gym_RLcourse\n",
        "!pip install -e .\n",
        "!pip install matplotlib\n",
        "!pip install pygame\n",
        "\n",
        "! pip install gymnasium 'gymnasium[atari]' 'gymnasium[accept-rom-license]'\n",
        "\n",
        "import gymnasium as gym\n",
        "import gym_RLcourse\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output # Used to clear the ouput of a Jupyter cell.\n",
        "\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "import math\n",
        "import random\n",
        "### Torch imports\n",
        "import torch\n",
        "# from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import argparse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBrBrHPAW7ul",
        "outputId": "797afcad-a062-4917-ef26-965568e551d3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[all] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (0.0.4)\n",
            "Requirement already satisfied: shimmy[atari]<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (0.2.1)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[all])\n",
            "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (2.5.2)\n",
            "Collecting swig==4.* (from gymnasium[all])\n",
            "  Using cached swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "Collecting mujoco-py<2.2,>=2.1 (from gymnasium[all])\n",
            "  Using cached mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\n",
            "Collecting cython<3 (from gymnasium[all])\n",
            "  Using cached Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "Collecting mujoco>=2.3.3 (from gymnasium[all])\n",
            "  Using cached mujoco-3.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (2.31.6)\n",
            "Requirement already satisfied: jax>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (0.4.26)\n",
            "Requirement already satisfied: jaxlib>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (0.4.26+cuda12.cudnn89)\n",
            "Collecting lz4>=3.1.0 (from gymnasium[all])\n",
            "  Using cached lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Requirement already satisfied: opencv-python>=3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (4.8.0.76)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (3.7.1)\n",
            "Requirement already satisfied: moviepy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (1.0.3)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (2.2.1+cu121)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.14.1->gymnasium[all]) (9.4.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.0->gymnasium[all]) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.0->gymnasium[all]) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.0->gymnasium[all]) (1.11.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (2.8.2)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (2.31.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (0.1.10)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (0.4.9)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[all]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[all]) (1.7.0)\n",
            "Collecting glfw (from mujoco>=2.3.3->gymnasium[all])\n",
            "  Using cached glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (211 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[all]) (3.1.7)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1->gymnasium[all]) (1.16.0)\n",
            "Collecting fasteners~=0.15 (from mujoco-py<2.2,>=2.1->gymnasium[all])\n",
            "  Using cached fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0->gymnasium[all]) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[all]) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[all]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[all]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[all]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[all]) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.0.0->gymnasium[all])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.0.0->gymnasium[all])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.0.0->gymnasium[all])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.0.0->gymnasium[all])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.0.0->gymnasium[all])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.0.0->gymnasium[all])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.0.0->gymnasium[all])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.0.0->gymnasium[all])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.0.0->gymnasium[all])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.0.0->gymnasium[all])\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.0.0->gymnasium[all])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[all]) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->gymnasium[all])\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[all]) (6.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1->gymnasium[all]) (2.22)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy>=1.0.0->gymnasium[all]) (67.7.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->gymnasium[all]) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (2024.2.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[all]) (3.18.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->gymnasium[all]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->gymnasium[all]) (1.3.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "\u001b[31mERROR: Could not build wheels for box2d-py, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.10).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "fatal: destination path 'gym_RLcourse' already exists and is not an empty directory.\n",
            "/content/gym_RLcourse/gym_RLcourse\n",
            "Obtaining file:///content/gym_RLcourse/gym_RLcourse\n",
            "\u001b[31mERROR: file:///content/gym_RLcourse/gym_RLcourse does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (2.5.2)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: shimmy[atari]<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.2.1)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium) (4.66.4)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium) (0.6.1)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0->gymnasium) (0.8.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium) (6.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fo-cIPrUU9mK"
      },
      "outputs": [],
      "source": [
        "## config.py\n",
        "\"\"\"\n",
        "In this file, you may edit the hyperparameters used for different environments.\n",
        "\n",
        "memory_size: Maximum size of the replay memory.\n",
        "n_episodes: Number of episodes to train for.\n",
        "batch_size: Batch size used for training DQN.\n",
        "target_update_frequency: How often to update the target network.\n",
        "train_frequency: How often to train the DQN.\n",
        "gamma: Discount factor.\n",
        "lr: Learning rate used for optimizer.\n",
        "eps_start: Starting value for epsilon (linear annealing).\n",
        "eps_end: Final value for epsilon (linear annealing).\n",
        "anneal_length: How many steps to anneal epsilon for.\n",
        "n_actions: The number of actions can easily be accessed with env.action_space.n, but we do\n",
        "    some manual engineering to account for the fact that Pong has duplicate actions.\n",
        "\"\"\"\n",
        "\n",
        "# Hyperparameters for CartPole-v1\n",
        "CartPole = {\n",
        "    'memory_size': 50000,\n",
        "    'n_episodes': 1000,\n",
        "    'batch_size': 32,\n",
        "    'target_update_frequency': 100,\n",
        "    'train_frequency': 1,\n",
        "    'gamma': 0.95,\n",
        "    'lr': 1e-4,\n",
        "    'eps_start': 1.0,\n",
        "    'eps_end': 0.05,\n",
        "    'anneal_length': 10**4,\n",
        "    'n_actions': 2,\n",
        "    'epsilon': 0.9\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## utils.py\n",
        "# import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def preprocess(obs, env):\n",
        "    \"\"\"Performs necessary observation preprocessing.\"\"\"\n",
        "    if env in ['CartPole-v1']:\n",
        "        return torch.tensor(obs, device=device).float()\n",
        "    else:\n",
        "        raise ValueError('Please add necessary observation preprocessing instructions to preprocess() in utils.py.')\n"
      ],
      "metadata": {
        "id": "myBc3GUnV-xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## dqn.py\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "    def push(self, obs, action, next_obs, reward):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "\n",
        "        self.memory[self.position] = (obs, action, next_obs, reward)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Samples batch_size transitions from the replay memory and returns a tuple\n",
        "            (obs, action, next_obs, reward)\n",
        "        \"\"\"\n",
        "        sample = random.sample(self.memory, batch_size)\n",
        "        return tuple(zip(*sample))\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, env_config):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        # Save hyperparameters needed in the DQN class.\n",
        "        self.batch_size = env_config[\"batch_size\"]\n",
        "        self.gamma = env_config[\"gamma\"]\n",
        "        self.eps_start = env_config[\"eps_start\"]\n",
        "        self.eps_end = env_config[\"eps_end\"]\n",
        "        self.anneal_length = env_config[\"anneal_length\"]\n",
        "        self.n_actions = env_config[\"n_actions\"]\n",
        "\n",
        "        self.epsilon = env_config[\"epsilon\"]\n",
        "\n",
        "        self.fc1 = nn.Linear(4, 256)\n",
        "        self.fc2 = nn.Linear(256, self.n_actions)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Runs the forward pass of the NN depending on architecture.\"\"\"\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def act(self, observation, exploit=False):\n",
        "        \"\"\"Selects an action with an epsilon-greedy exploration strategy.\"\"\"\n",
        "        # TODO: Implement action selection using the Deep Q-network. This function\n",
        "        #       takes an observation tensor and should return a tensor of actions.\n",
        "        #       For example, if the state dimension is 4 and the batch size is 32,\n",
        "        #       the input would be a [32, 4] tensor and the output a [32, 1] tensor.\n",
        "        # TODO: Implement epsilon-greedy exploration.\n",
        "\n",
        "        # Ensure observation is a PyTorch tensor\n",
        "        observation = torch.tensor(observation)\n",
        "\n",
        "        # Pass observation through DQN to get Q-values\n",
        "        q_values = self.forward(observation)\n",
        "\n",
        "        # Explore with epsilon probability\n",
        "        if random.random() < self.epsilon:\n",
        "            # Choose a random action\n",
        "            action = torch.randint(0, q_values.size(1), (1,))\n",
        "        else:\n",
        "            # Choose the action with the highest Q-value\n",
        "            action = q_values.argmax(dim=1)\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "        raise NotImplmentedError\n",
        "\n",
        "def optimize(dqn, target_dqn, memory, optimizer):\n",
        "    \"\"\"This function samples a batch from the replay buffer and optimizes the Q-network.\"\"\"\n",
        "    # If we don't have enough transitions stored yet, we don't train.\n",
        "    if len(memory) < dqn.batch_size:\n",
        "        return\n",
        "\n",
        "    # TODO: Sample a batch from the replay memory and concatenate so that there are\n",
        "    #       four tensors in total: observations, actions, next observations and rewards.\n",
        "    #       Remember to move them to GPU if it is available, e.g., by using Tensor.to(device).\n",
        "    #       Note that special care is needed for terminal transitions!\n",
        "    observations, actions, next_observations, rewards = memory.sample()\n",
        "    observations.to(device)\n",
        "    actions.to(device)\n",
        "    next_observations.to(device)\n",
        "    rewards.to(device)\n",
        "\n",
        "    # TODO: Compute the current estimates of the Q-values for each state-action\n",
        "    #       pair (s,a). Here, torch.gather() is useful for selecting the Q-values\n",
        "    #       corresponding to the chosen actions.\n",
        "\n",
        "    # TODO: Compute the Q-value targets. Only do this for non-terminal transitions!\n",
        "\n",
        "    # Compute Q-values for the current state-action pairs\n",
        "    q_values = dqn(observations)\n",
        "    q_values = torch.gather(q_values, 1, actions.unsqueeze(1))\n",
        "\n",
        "    # Compute Q-values for the next states\n",
        "    next_q_values = target_dqn(next_observations)\n",
        "    next_q_values = next_q_values.max(1)[0].detach()\n",
        "\n",
        "    q_value_targets = rewards + dqn.gamma * next_q_values * (1 - terminals)\n",
        "    # Compute loss.\n",
        "    loss = F.mse_loss(q_values.squeeze(), q_value_targets)\n",
        "\n",
        "    # Perform gradient descent.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n"
      ],
      "metadata": {
        "id": "zZCio_OrV_Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## train.py\n",
        "\n",
        "\n",
        "# import gymnasium as gym\n",
        "# import torch\n",
        "\n",
        "# import copy\n",
        "\n",
        "# import config\n",
        "# from utils import preprocess\n",
        "# from evaluate import evaluate_policy\n",
        "# from dqn import DQN, ReplayMemory, optimize\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument('--env', choices=['CartPole-v1'], default='CartPole-v1')\n",
        "# parser.add_argument('--evaluate_freq', type=int, default=25, help='How often to run evaluation.', nargs='?')\n",
        "# parser.add_argument('--evaluation_episodes', type=int, default=5, help='Number of evaluation episodes.', nargs='?')\n",
        "args = argparse.Namespace(\n",
        "    env='CartPole-v1',\n",
        "    evaluate_freq=5,\n",
        "    evaluation_episodes=1000\n",
        ")\n",
        "\n",
        "\n",
        "# Hyperparameter configurations for different environments. See config.py.\n",
        "ENV_CONFIGS = {\n",
        "    'CartPole-v1': config.CartPole\n",
        "}\n",
        "\n",
        "def evaluate_policy(dqn, env, env_config, args, n_episodes, render=False, verbose=False):\n",
        "    \"\"\"Runs {n_episodes} episodes to evaluate current policy.\"\"\"\n",
        "    total_return = 0\n",
        "    for i in range(n_episodes):\n",
        "        obs, info = env.reset()\n",
        "        obs = preprocess(obs, env=args.env).unsqueeze(0)\n",
        "\n",
        "        terminated = False\n",
        "        episode_return = 0\n",
        "\n",
        "        while not terminated:\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "            action = dqn.act(obs, exploit=True)\n",
        "            obs, reward, terminated, truncated, info = env.step(action)\n",
        "            obs = preprocess(obs, env=args.env).unsqueeze(0)\n",
        "\n",
        "            episode_return += reward\n",
        "\n",
        "        total_return += episode_return\n",
        "\n",
        "        if verbose:\n",
        "            print(f'Finished episode {i+1} with a total return of {episode_return}')\n",
        "\n",
        "\n",
        "    return total_return / n_episodes\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # args = parser.parse_args()\n",
        "\n",
        "    # Initialize environment and config.\n",
        "    env = gym.make(args.env)\n",
        "    env_config = ENV_CONFIGS[args.env]\n",
        "\n",
        "    # Initialize deep Q-networks.\n",
        "    dqn = DQN(env_config=env_config).to(device)\n",
        "\n",
        "    # TODO: Create and initialize target Q-network.\n",
        "    dqn_target = copy.deepcopy(dqn)\n",
        "    # Create replay memory.\n",
        "    memory = ReplayMemory(env_config['memory_size'])\n",
        "\n",
        "    # Initialize optimizer used for training the DQN. We use Adam rather than RMSProp.\n",
        "    optimizer = torch.optim.Adam(dqn.parameters(), lr=env_config['lr'])\n",
        "\n",
        "    # Keep track of best evaluation mean return achieved so far.\n",
        "    best_mean_return = -float(\"Inf\")\n",
        "\n",
        "    for episode in range(env_config['n_episodes']):\n",
        "        terminated = False\n",
        "        obs, info = env.reset()\n",
        "        if episode == 0:\n",
        "            print(type(obs))\n",
        "            print(obs)\n",
        "\n",
        "\n",
        "        obs = preprocess(obs, env=args.env).unsqueeze(0)\n",
        "        if episode == 0:\n",
        "            print(type(obs))\n",
        "            print(obs)\n",
        "        step = 1\n",
        "        while not terminated:\n",
        "            # TODO: Get action from DQN.\n",
        "            ##action = None\n",
        "            action = dqn.act(obs).item()\n",
        "\n",
        "\n",
        "            # Act in the true environment.\n",
        "            old_obs = obs.clone().detach()\n",
        "            obs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            # Preprocess incoming observation.\n",
        "            if not terminated:\n",
        "                obs = preprocess(obs, env=args.env).unsqueeze(0)\n",
        "\n",
        "            # TODO: Add the transition to the replay memory. Remember to convert\n",
        "            #       everything to PyTorch tensors!\n",
        "            memory.push(torch.tensor(old_obs), torch.tensor(action), torch.tensor(obs), torch.tensor(reward))\n",
        "            # TODO: Run DQN.optimize() every env_config[\"train_frequency\"] steps.\n",
        "            if step%env_config[\"train_frequency\"] == 0:\n",
        "                pass\n",
        "            # TODO: Update the target network every env_config[\"target_update_frequency\"] steps.\n",
        "            if step%env_config[\"target_update_frequency\"] == 0:\n",
        "                pass\n",
        "            step += 1\n",
        "\n",
        "        # Evaluate the current agent.\n",
        "        if episode % args.evaluate_freq == 0:\n",
        "            mean_return = evaluate_policy(dqn, env, env_config, args, n_episodes=args.evaluation_episodes)\n",
        "            print(f'Episode {episode+1}/{env_config[\"n_episodes\"]}: {mean_return}')\n",
        "\n",
        "            # Save current agent if it has the best performance so far.\n",
        "            if mean_return >= best_mean_return:\n",
        "                best_mean_return = mean_return\n",
        "\n",
        "                print('Best performance so far! Saving model.')\n",
        "                torch.save(dqn, f'models/{args.env}_best.pt')\n",
        "\n",
        "    # Close environment after training is completed.\n",
        "    env.close()\n"
      ],
      "metadata": {
        "id": "_jlmnjAIV_WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## evaluate.py\n",
        "\n",
        "import argparse\n",
        "\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "\n",
        "import config\n",
        "from utils import preprocess\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# parser = argparse.ArgumentParser()\n",
        "# parser.add_argument('--env', choices=['CartPole-v1'], default='CartPole-v1')\n",
        "# parser.add_argument('--path', type=str, help='Path to stored DQN model.')\n",
        "# parser.add_argument('--n_eval_episodes', type=int, default=1, help='Number of evaluation episodes.', nargs='?')\n",
        "# parser.add_argument('--render', dest='render', action='store_true', help='Render the environment.')\n",
        "# parser.add_argument('--save_video', dest='save_video', action='store_true', help='Save the episodes as video.')\n",
        "# parser.set_defaults(render=False)\n",
        "# parser.set_defaults(save_video=False)\n",
        "args = argparse.Namespace(\n",
        "    # env='CartPole-v1',\n",
        "    # evaluate_freq=25,\n",
        "    # evaluation_episodes=5\n",
        "    env = 'CartPole-v1',\n",
        "    path = 'models/CartPole-v1_best.pt',\n",
        "    n_eval_episodes = 1000,\n",
        "    render = False,\n",
        "    save_video = False\n",
        ")\n",
        "\n",
        "\n",
        "# Hyperparameter configurations for different environments. See config.py.\n",
        "ENV_CONFIGS = {\n",
        "    'CartPole-v1': config.CartPole,\n",
        "}\n",
        "\n",
        "\n",
        "def evaluate_policy(dqn, env, env_config, args, n_episodes, render=False, verbose=False):\n",
        "    \"\"\"Runs {n_episodes} episodes to evaluate current policy.\"\"\"\n",
        "    total_return = 0\n",
        "    for i in range(n_episodes):\n",
        "        obs, info = env.reset()\n",
        "        obs = preprocess(obs, env=args.env).unsqueeze(0)\n",
        "\n",
        "        terminated = False\n",
        "        episode_return = 0\n",
        "\n",
        "        while not terminated:\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "            action = dqn.act(obs, exploit=True).item()\n",
        "            obs, reward, terminated, truncated, info = env.step(action)\n",
        "            obs = preprocess(obs, env=args.env).unsqueeze(0)\n",
        "\n",
        "            episode_return += reward\n",
        "\n",
        "        total_return += episode_return\n",
        "\n",
        "        if verbose:\n",
        "            print(f'Finished episode {i+1} with a total return of {episode_return}')\n",
        "\n",
        "\n",
        "    return total_return / n_episodes\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # args = parser.parse_args()\n",
        "\n",
        "    # Initialize environment and config\n",
        "    env = gym.make(args.env)\n",
        "    env_config = ENV_CONFIGS[args.env]\n",
        "\n",
        "    if args.save_video:\n",
        "        env = gym.make(args.env, render_mode='rgb_array')\n",
        "        env = gym.wrappers.RecordVideo(env, './video/', episode_trigger=lambda episode_id: True)\n",
        "\n",
        "    # Load model from provided path.\n",
        "    dqn = torch.load(args.path, map_location=torch.device('cpu'))\n",
        "    dqn.eval()\n",
        "\n",
        "    mean_return = evaluate_policy(dqn, env, env_config, args, args.n_eval_episodes, render=args.render and not args.save_video, verbose=True)\n",
        "    print(f'The policy got a mean return of {mean_return} over {args.n_eval_episodes} episodes.')\n",
        "\n",
        "    env.close()"
      ],
      "metadata": {
        "id": "xPcVQLfjV_b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N3ajWyHQV_g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OPxK23TWV_lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oZCV3TRLV_qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wsPrBvusV_uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EbAbafjQV_y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OfdP-Y3RV_3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JiUXb4XqV_6q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}