{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UbFelb_5bIwz",
    "outputId": "ec468bda-4d14-42e0-ee40-f9cdcdabf854"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium[all]\n",
      "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (1.25.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (4.11.0)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium[all])\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[all])\n",
      "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting box2d-py==2.3.5 (from gymnasium[all])\n",
      "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (2.5.2)\n",
      "Collecting swig==4.* (from gymnasium[all])\n",
      "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mujoco-py<2.2,>=2.1 (from gymnasium[all])\n",
      "  Downloading mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cython<3 (from gymnasium[all])\n",
      "  Downloading Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mujoco>=2.3.3 (from gymnasium[all])\n",
      "  Downloading mujoco-3.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (2.31.6)\n",
      "Requirement already satisfied: jax>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (0.4.26)\n",
      "Requirement already satisfied: jaxlib>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (0.4.26+cuda12.cudnn89)\n",
      "Collecting lz4>=3.1.0 (from gymnasium[all])\n",
      "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: opencv-python>=3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (4.8.0.76)\n",
      "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (3.7.1)\n",
      "Requirement already satisfied: moviepy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (1.0.3)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[all]) (2.2.1+cu121)\n",
      "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.14.1->gymnasium[all]) (9.4.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.0->gymnasium[all]) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.0->gymnasium[all]) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.0->gymnasium[all]) (1.11.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (24.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gymnasium[all]) (2.8.2)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (2.31.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (0.1.10)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy>=1.0.0->gymnasium[all]) (0.4.9)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[all]) (1.4.0)\n",
      "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[all]) (1.7.0)\n",
      "Collecting glfw (from mujoco>=2.3.3->gymnasium[all])\n",
      "  Downloading glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (211 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.3.3->gymnasium[all]) (3.1.7)\n",
      "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1->gymnasium[all]) (1.16.0)\n",
      "Collecting fasteners~=0.15 (from mujoco-py<2.2,>=2.1->gymnasium[all])\n",
      "  Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
      "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[all])\n",
      "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[all]) (3.14.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[all]) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[all]) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[all]) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[all]) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.0.0->gymnasium[all])\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.0.0->gymnasium[all])\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.0.0->gymnasium[all])\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.0.0->gymnasium[all])\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.0.0->gymnasium[all])\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.0.0->gymnasium[all])\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.0.0->gymnasium[all])\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.0.0->gymnasium[all])\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.0.0->gymnasium[all])\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.0.0->gymnasium[all])\n",
      "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.0.0->gymnasium[all])\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->gymnasium[all]) (2.2.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->gymnasium[all])\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[all]) (6.4.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1->gymnasium[all]) (2.22)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy>=1.0.0->gymnasium[all]) (67.7.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->gymnasium[all]) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0->gymnasium[all]) (2024.2.2)\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.3.3->gymnasium[all]) (3.18.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->gymnasium[all]) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->gymnasium[all]) (1.3.0)\n",
      "Building wheels for collected packages: box2d-py\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
      "Failed to build box2d-py\n",
      "\u001b[31mERROR: Could not build wheels for box2d-py, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "git is already the newest version (1:2.34.1-1ubuntu1.10).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
      "Cloning into 'gym_RLcourse'...\n",
      "remote: Enumerating objects: 82, done.\u001b[K\n",
      "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
      "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
      "remote: Total 82 (delta 35), reused 51 (delta 17), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (82/82), 37.11 KiB | 18.56 MiB/s, done.\n",
      "Resolving deltas: 100% (35/35), done.\n",
      "/content/gym_RLcourse\n",
      "Obtaining file:///content/gym_RLcourse\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting gymnasium (from gym-RLcourse==0.0.3)\n",
      "  Using cached gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gym-RLcourse==0.0.3) (1.25.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium->gym-RLcourse==0.0.3) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium->gym-RLcourse==0.0.3) (4.11.0)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium->gym-RLcourse==0.0.3)\n",
      "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, gymnasium, gym-RLcourse\n",
      "  Running setup.py develop for gym-RLcourse\n",
      "Successfully installed farama-notifications-0.0.4 gym-RLcourse-0.0.3 gymnasium-0.29.1\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (2.5.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install gymnasium[all]\n",
    "!apt-get install git\n",
    "!git clone https://github.com/magni84/gym_RLcourse.git\n",
    "%cd gym_RLcourse\n",
    "!pip install -e .\n",
    "!pip install matplotlib\n",
    "!pip install pygame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cI34_iQLdiay",
    "outputId": "5c8f25c2-a985-476c-bf18-154289c4cde2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
      "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium)\n",
      "  Using cached Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium)\n",
      "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium) (8.1.7)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium) (4.66.4)\n",
      "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium)\n",
      "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium)\n",
      "  Using cached ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium) (6.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium) (2024.2.2)\n",
      "Building wheels for collected packages: AutoROM.accept-rom-license\n",
      "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446659 sha256=f2cbba3d09c2c9a4a383203bc1b932d7a5e2dfdcaa7d46b3ce09fcd550dad0ec\n",
      "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
      "Successfully built AutoROM.accept-rom-license\n",
      "Installing collected packages: ale-py, shimmy, AutoROM.accept-rom-license, autorom\n",
      "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 shimmy-0.2.1\n"
     ]
    }
   ],
   "source": [
    "! pip install gymnasium 'gymnasium[atari]' 'gymnasium[accept-rom-license]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KocTM7Y5dreU"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gym_RLcourse\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output # Used to clear the ouput of a Jupyter cell.\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import math\n",
    "import random\n",
    "### Torch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zZbAPpFIdCGi",
    "outputId": "eb79f40f-cb38-448f-991b-1ba9b5c2f9d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('ALE/Pong-v5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RY07QtHXiex0"
   },
   "outputs": [],
   "source": [
    "## Config.py\n",
    "\"\"\"\n",
    "In this file, you may edit the hyperparameters used for different environments.\n",
    "\n",
    "memory_size: Maximum size of the replay memory.\n",
    "n_episodes: Number of episodes to train for.\n",
    "batch_size: Batch size used for training DQN.\n",
    "target_update_frequency: How often to update the target network.\n",
    "train_frequency: How often to train the DQN.\n",
    "gamma: Discount factor.\n",
    "lr: Learning rate used for optimizer.\n",
    "eps_start: Starting value for epsilon (linear annealing).\n",
    "eps_end: Final value for epsilon (linear annealing).\n",
    "anneal_length: How many steps to anneal epsilon for.\n",
    "n_actions: The number of actions can easily be accessed with env.action_space.n, but we do\n",
    "    some manual engineering to account for the fact that Pong has duplicate actions.\n",
    "\"\"\"\n",
    "\n",
    "# Hyperparameters for CartPole-v1\n",
    "CartPole = {\n",
    "    'memory_size': 50000,\n",
    "    'n_episodes': 1000,\n",
    "    'batch_size': 32,\n",
    "    'target_update_frequency': 100,\n",
    "    'train_frequency': 1,\n",
    "    'gamma': 0.95,\n",
    "    'lr': 1e-4,\n",
    "    'eps_start': 1.0,\n",
    "    'eps_end': 0.05,\n",
    "    'anneal_length': 10**4,\n",
    "    'n_actions': 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZNHp_n_2iS0r"
   },
   "outputs": [],
   "source": [
    "## Utils.py\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def preprocess(obs, env):\n",
    "    \"\"\"Performs necessary observation preprocessing.\"\"\"\n",
    "    if env in ['CartPole-v1']:\n",
    "        return torch.tensor(obs, device=device).float()\n",
    "    else:\n",
    "        raise ValueError('Please add necessary observation preprocessing instructions to preprocess() in utils.py.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GeT4Ru2_ivHT",
    "outputId": "2d689c7f-5c7e-480a-a513-4b15f1bce6be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "## dqn.py\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def push(self, obs, action, next_obs, reward):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "\n",
    "        self.memory[self.position] = (obs, action, next_obs, reward)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Samples batch_size transitions from the replay memory and returns a tuple\n",
    "            (obs, action, next_obs, reward)\n",
    "        \"\"\"\n",
    "        sample = random.sample(self.memory, batch_size)\n",
    "        return tuple(zip(*sample))\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, env_config):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        # Save hyperparameters needed in the DQN class.\n",
    "        self.batch_size = env_config[\"batch_size\"]\n",
    "        self.gamma = env_config[\"gamma\"]\n",
    "        self.eps_start = env_config[\"eps_start\"]\n",
    "        self.eps_end = env_config[\"eps_end\"]\n",
    "        self.anneal_length = env_config[\"anneal_length\"]\n",
    "        self.n_actions = env_config[\"n_actions\"]\n",
    "\n",
    "        self.fc1 = nn.Linear(4, 256)\n",
    "        self.fc2 = nn.Linear(256, self.n_actions)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Runs the forward pass of the NN depending on architecture.\"\"\"\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def act(self, observation, exploit=False):\n",
    "        \"\"\"Selects an action with an epsilon-greedy exploration strategy.\"\"\"\n",
    "        # TODO: Implement action selection using the Deep Q-network. This function\n",
    "        #       takes an observation tensor and should return a tensor of actions.\n",
    "        #       For example, if the state dimension is 4 and the batch size is 32,\n",
    "        #       the input would be a [32, 4] tensor and the output a [32, 1] tensor.\n",
    "        # TODO: Implement epsilon-greedy exploration.\n",
    "        # return action\n",
    "        if not exploit and random.random() < self.eps_end:\n",
    "            # Explore action space\n",
    "            action = random.randrange(self.n_actions)\n",
    "        else:\n",
    "            # Exploit learned values\n",
    "            with torch.no_grad():\n",
    "                action_values = self.forward(observation)\n",
    "            action = torch.argmax(action_values, dim=1).item()\n",
    "\n",
    "        return action\n",
    "        #raise NotImplmentedError\n",
    "\n",
    "def optimize(dqn, target_dqn, memory, optimizer):\n",
    "    \"\"\"This function samples a batch from the replay buffer and optimizes the Q-network.\"\"\"\n",
    "    # If we don't have enough transitions stored yet, we don't train.\n",
    "    if len(memory) < dqn.batch_size:\n",
    "        return\n",
    "\n",
    "    # TODO: Sample a batch from the replay memory and concatenate so that there are\n",
    "    #       four tensors in total: observations, actions, next observations and rewards.\n",
    "    #       Remember to move them to GPU if it is available, e.g., by using Tensor.to(device).\n",
    "    #       Note that special care is needed for terminal transitions!\n",
    "\n",
    "    # TODO: Compute the current estimates of the Q-values for each state-action\n",
    "    #       pair (s,a). Here, torch.gather() is useful for selecting the Q-values\n",
    "    #       corresponding to the chosen actions.\n",
    "\n",
    "    # TODO: Compute the Q-value targets. Only do this for non-terminal transitions!\n",
    "    batch = memory.sample(dqn.batch_size)\n",
    "    print(\"batch:\", batch)\n",
    "    print(\"len(batch)\", len(batch))\n",
    "    cum_loss = 0.\n",
    "    obs_ = batch[0]\n",
    "    action_ = batch[1]\n",
    "    next_obs_ = batch[2]\n",
    "    reward_ = batch[3]\n",
    "    print(\"next obs list\", next_obs_)\n",
    "    for i in range(len(batch)):\n",
    "      # print(f\"Element {idx}: {x}, type: {type(x)}, length: {len(x)}\")\n",
    "\n",
    "      # Move tensors to device\n",
    "      obs = obs_[i].to(delvice)\n",
    "\n",
    "      next_obs = next_obs_[i]\n",
    "      if not torch.is_tensor(next_obs):\n",
    "        next_obs = torch.tensor(next_obs).to(device)\n",
    "      print(next_obs)\n",
    "      reward = reward_[i]\n",
    "      action = torch.tensor(action_[i]).unsqueeze(0).to(device)\n",
    "      # Compute the current estimates of the Q-values for each state-action pair\n",
    "\n",
    "      print(\"obs, action\", obs, action.unsqueeze(-1))\n",
    "      q_values = dqn(obs).gather(1, action.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "      # Compute the Q-value targets for non-terminal transitions\n",
    "      with torch.no_grad():\n",
    "        next_q_values = target_dqn(next_obs).max(dim=1)[0].detach()\n",
    "        q_value_targets = reward + dqn.gamma * next_q_values\n",
    "\n",
    "      # Compute loss.\n",
    "      loss = F.mse_loss(q_values.squeeze(), q_value_targets)\n",
    "      cum_loss += loss\n",
    "      # Perform gradient descent.\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "    return cum_loss/len(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "BPUPn-Ile5qF",
    "outputId": "62142b8d-88da-4609-f97e-48c61c26fc6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000: 9.342\n",
      "Best performance so far! Saving model.\n",
      "Episode 3/1000: 9.348\n",
      "Best performance so far! Saving model.\n",
      "batch: ((tensor([[-0.0458, -1.3442,  0.1132,  2.1032]], device='cuda:0'), tensor([[-0.1053, -1.2192,  0.1317,  1.8581]], device='cuda:0'), tensor([[ 0.0226, -0.5620,  0.0065,  0.8854]], device='cuda:0'), tensor([[ 0.0025, -0.3772, -0.0041,  0.5575]], device='cuda:0'), tensor([[ 0.0113, -0.7572,  0.0242,  1.1801]], device='cuda:0'), tensor([[-0.0050, -0.5722,  0.0070,  0.8489]], device='cuda:0'), tensor([[-0.0413, -0.0441,  0.0406, -0.0078]], device='cuda:0'), tensor([[ 0.0299, -0.3670, -0.0054,  0.5944]], device='cuda:0'), tensor([[-0.0849, -1.0231,  0.1010,  1.5356]], device='cuda:0'), tensor([[-0.1035, -1.7364,  0.2038,  2.7646]], device='cuda:0'), tensor([[ 0.0103, -0.8267,  0.0336,  1.1976]], device='cuda:0'), tensor([[-0.0267, -1.2180,  0.0875,  1.8107]], device='cuda:0'), tensor([[ 0.0374, -0.0460, -0.0033,  0.0229]], device='cuda:0'), tensor([[-0.0165, -0.7674,  0.0240,  1.1438]], device='cuda:0'), tensor([[-0.0063, -1.0222,  0.0575,  1.5006]], device='cuda:0'), tensor([[-0.0422, -0.2398,  0.0404,  0.2974]], device='cuda:0'), tensor([[ 0.0329,  0.0229, -0.0118,  0.0164]], device='cuda:0'), tensor([[ 0.0364, -0.2411, -0.0029,  0.3145]], device='cuda:0'), tensor([[-0.0038, -0.9527,  0.0478,  1.4803]], device='cuda:0'), tensor([[-0.0229, -1.1483,  0.0774,  1.7875]], device='cuda:0'), tensor([[-0.1297, -1.4155,  0.1689,  2.1886]], device='cuda:0'), tensor([[ 0.0062, -0.1822, -0.0095,  0.2678]], device='cuda:0'), tensor([[-0.0470, -0.4354,  0.0464,  0.6026]], device='cuda:0'), tensor([[-0.0683, -0.8270,  0.0766,  1.2199]], device='cuda:0'), tensor([[-0.0511, -1.4140,  0.1237,  2.1292]], device='cuda:0'), tensor([[ 0.0333, -0.1720, -0.0115,  0.3053]], device='cuda:0'), tensor([[-0.0793, -1.6101,  0.1663,  2.4575]], device='cuda:0'), tensor([[ 0.0229, -0.6313,  0.0156,  0.9001]], device='cuda:0'), tensor([[-0.0727, -1.5403,  0.1552,  2.4286]], device='cuda:0'), tensor([[ 0.0059,  0.0128, -0.0090, -0.0220]], device='cuda:0'), tensor([[-0.0557, -0.6312,  0.0584,  0.9095]], device='cuda:0'), tensor([[ 0.0316, -0.4362,  0.0034,  0.6063]], device='cuda:0')), (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), (tensor([[-0.0727, -1.5403,  0.1552,  2.4286]], device='cuda:0'), tensor([[-0.1297, -1.4155,  0.1689,  2.1886]], device='cuda:0'), tensor([[ 0.0113, -0.7572,  0.0242,  1.1801]], device='cuda:0'), tensor([[-0.0050, -0.5722,  0.0070,  0.8489]], device='cuda:0'), tensor([[-0.0038, -0.9527,  0.0478,  1.4803]], device='cuda:0'), tensor([[-0.0165, -0.7674,  0.0240,  1.1438]], device='cuda:0'), tensor([[-0.0422, -0.2398,  0.0404,  0.2974]], device='cuda:0'), tensor([[ 0.0226, -0.5620,  0.0065,  0.8854]], device='cuda:0'), tensor([[-0.1053, -1.2192,  0.1317,  1.8581]], device='cuda:0'), array([-0.1382595 , -1.9322416 ,  0.25909415,  3.111862  ], dtype=float32), tensor([[-0.0063, -1.0222,  0.0575,  1.5006]], device='cuda:0'), tensor([[-0.0511, -1.4140,  0.1237,  2.1292]], device='cuda:0'), tensor([[ 0.0364, -0.2411, -0.0029,  0.3145]], device='cuda:0'), tensor([[-0.0318, -0.9629,  0.0469,  1.4439]], device='cuda:0'), tensor([[-0.0267, -1.2180,  0.0875,  1.8107]], device='cuda:0'), tensor([[-0.0470, -0.4354,  0.0464,  0.6026]], device='cuda:0'), tensor([[ 0.0333, -0.1720, -0.0115,  0.3053]], device='cuda:0'), tensor([[ 0.0316, -0.4362,  0.0034,  0.6063]], device='cuda:0'), tensor([[-0.0229, -1.1483,  0.0774,  1.7875]], device='cuda:0'), tensor([[-0.0458, -1.3442,  0.1132,  2.1032]], device='cuda:0'), array([-0.15803663, -1.6118481 ,  0.21267907,  2.5282636 ], dtype=float32), tensor([[ 0.0025, -0.3772, -0.0041,  0.5575]], device='cuda:0'), tensor([[-0.0557, -0.6312,  0.0584,  0.9095]], device='cuda:0'), tensor([[-0.0849, -1.0231,  0.1010,  1.5356]], device='cuda:0'), tensor([[-0.0793, -1.6101,  0.1663,  2.4575]], device='cuda:0'), tensor([[ 0.0299, -0.3670, -0.0054,  0.5944]], device='cuda:0'), array([-0.11154171, -1.8061751 ,  0.21547833,  2.796214  ], dtype=float32), tensor([[ 0.0103, -0.8267,  0.0336,  1.1976]], device='cuda:0'), tensor([[-0.1035, -1.7364,  0.2038,  2.7646]], device='cuda:0'), tensor([[ 0.0062, -0.1822, -0.0095,  0.2678]], device='cuda:0'), tensor([[-0.0683, -0.8270,  0.0766,  1.2199]], device='cuda:0'), tensor([[ 0.0229, -0.6313,  0.0156,  0.9001]], device='cuda:0')), (1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0))\n",
      "len(batch) 4\n",
      "next obs list (tensor([[-0.0727, -1.5403,  0.1552,  2.4286]], device='cuda:0'), tensor([[-0.1297, -1.4155,  0.1689,  2.1886]], device='cuda:0'), tensor([[ 0.0113, -0.7572,  0.0242,  1.1801]], device='cuda:0'), tensor([[-0.0050, -0.5722,  0.0070,  0.8489]], device='cuda:0'), tensor([[-0.0038, -0.9527,  0.0478,  1.4803]], device='cuda:0'), tensor([[-0.0165, -0.7674,  0.0240,  1.1438]], device='cuda:0'), tensor([[-0.0422, -0.2398,  0.0404,  0.2974]], device='cuda:0'), tensor([[ 0.0226, -0.5620,  0.0065,  0.8854]], device='cuda:0'), tensor([[-0.1053, -1.2192,  0.1317,  1.8581]], device='cuda:0'), array([-0.1382595 , -1.9322416 ,  0.25909415,  3.111862  ], dtype=float32), tensor([[-0.0063, -1.0222,  0.0575,  1.5006]], device='cuda:0'), tensor([[-0.0511, -1.4140,  0.1237,  2.1292]], device='cuda:0'), tensor([[ 0.0364, -0.2411, -0.0029,  0.3145]], device='cuda:0'), tensor([[-0.0318, -0.9629,  0.0469,  1.4439]], device='cuda:0'), tensor([[-0.0267, -1.2180,  0.0875,  1.8107]], device='cuda:0'), tensor([[-0.0470, -0.4354,  0.0464,  0.6026]], device='cuda:0'), tensor([[ 0.0333, -0.1720, -0.0115,  0.3053]], device='cuda:0'), tensor([[ 0.0316, -0.4362,  0.0034,  0.6063]], device='cuda:0'), tensor([[-0.0229, -1.1483,  0.0774,  1.7875]], device='cuda:0'), tensor([[-0.0458, -1.3442,  0.1132,  2.1032]], device='cuda:0'), array([-0.15803663, -1.6118481 ,  0.21267907,  2.5282636 ], dtype=float32), tensor([[ 0.0025, -0.3772, -0.0041,  0.5575]], device='cuda:0'), tensor([[-0.0557, -0.6312,  0.0584,  0.9095]], device='cuda:0'), tensor([[-0.0849, -1.0231,  0.1010,  1.5356]], device='cuda:0'), tensor([[-0.0793, -1.6101,  0.1663,  2.4575]], device='cuda:0'), tensor([[ 0.0299, -0.3670, -0.0054,  0.5944]], device='cuda:0'), array([-0.11154171, -1.8061751 ,  0.21547833,  2.796214  ], dtype=float32), tensor([[ 0.0103, -0.8267,  0.0336,  1.1976]], device='cuda:0'), tensor([[-0.1035, -1.7364,  0.2038,  2.7646]], device='cuda:0'), tensor([[ 0.0062, -0.1822, -0.0095,  0.2678]], device='cuda:0'), tensor([[-0.0683, -0.8270,  0.0766,  1.2199]], device='cuda:0'), tensor([[ 0.0229, -0.6313,  0.0156,  0.9001]], device='cuda:0'))\n",
      "tensor([[-0.0727, -1.5403,  0.1552,  2.4286]], device='cuda:0')\n",
      "obs, action tensor([[-0.0458, -1.3442,  0.1132,  2.1032]], device='cuda:0') tensor([[0]], device='cuda:0')\n",
      "tensor([[-0.1297, -1.4155,  0.1689,  2.1886]], device='cuda:0')\n",
      "obs, action tensor([[-0.1053, -1.2192,  0.1317,  1.8581]], device='cuda:0') tensor([[0]], device='cuda:0')\n",
      "tensor([[ 0.0113, -0.7572,  0.0242,  1.1801]], device='cuda:0')\n",
      "obs, action tensor([[ 0.0226, -0.5620,  0.0065,  0.8854]], device='cuda:0') tensor([[0]], device='cuda:0')\n",
      "tensor([[-0.0050, -0.5722,  0.0070,  0.8489]], device='cuda:0')\n",
      "obs, action tensor([[ 0.0025, -0.3772, -0.0041,  0.5575]], device='cuda:0') tensor([[0]], device='cuda:0')\n",
      "batch: ((tensor([[-0.0267, -1.2180,  0.0875,  1.8107]], device='cuda:0'), tensor([[-0.0683, -0.8270,  0.0766,  1.2199]], device='cuda:0'), tensor([[ 0.0103, -0.8267,  0.0336,  1.1976]], device='cuda:0'), tensor([[-0.1035, -1.7364,  0.2038,  2.7646]], device='cuda:0'), tensor([[ 0.0374, -0.0460, -0.0033,  0.0229]], device='cuda:0'), tensor([[ 0.0229, -0.6313,  0.0156,  0.9001]], device='cuda:0'), tensor([[ 0.0316, -0.4362,  0.0034,  0.6063]], device='cuda:0'), tensor([[-0.0050, -0.5722,  0.0070,  0.8489]], device='cuda:0'), tensor([[-0.0063, -1.0222,  0.0575,  1.5006]], device='cuda:0'), tensor([[-0.0038, -0.9527,  0.0478,  1.4803]], device='cuda:0'), tensor([[-0.0511, -1.4140,  0.1237,  2.1292]], device='cuda:0'), tensor([[-0.0165, -0.7674,  0.0240,  1.1438]], device='cuda:0'), tensor([[ 0.0025, -0.3772, -0.0041,  0.5575]], device='cuda:0'), tensor([[ 0.0113, -0.7572,  0.0242,  1.1801]], device='cuda:0'), tensor([[ 0.0329,  0.0229, -0.0118,  0.0164]], device='cuda:0'), tensor([[-0.0413, -0.0441,  0.0406, -0.0078]], device='cuda:0'), tensor([[-0.0557, -0.6312,  0.0584,  0.9095]], device='cuda:0'), tensor([[-0.1297, -1.4155,  0.1689,  2.1886]], device='cuda:0'), tensor([[ 0.0062, -0.1822, -0.0095,  0.2678]], device='cuda:0'), tensor([[-0.0229, -1.1483,  0.0774,  1.7875]], device='cuda:0'), tensor([[-0.0458, -1.3442,  0.1132,  2.1032]], device='cuda:0'), tensor([[-0.0422, -0.2398,  0.0404,  0.2974]], device='cuda:0'), tensor([[ 0.0364, -0.2411, -0.0029,  0.3145]], device='cuda:0'), tensor([[ 0.0059,  0.0128, -0.0090, -0.0220]], device='cuda:0'), tensor([[-0.0470, -0.4354,  0.0464,  0.6026]], device='cuda:0'), tensor([[ 0.0299, -0.3670, -0.0054,  0.5944]], device='cuda:0'), tensor([[-0.0793, -1.6101,  0.1663,  2.4575]], device='cuda:0'), tensor([[-0.0849, -1.0231,  0.1010,  1.5356]], device='cuda:0'), tensor([[-0.1053, -1.2192,  0.1317,  1.8581]], device='cuda:0'), tensor([[ 0.0226, -0.5620,  0.0065,  0.8854]], device='cuda:0'), tensor([[-0.0727, -1.5403,  0.1552,  2.4286]], device='cuda:0'), tensor([[ 0.0333, -0.1720, -0.0115,  0.3053]], device='cuda:0')), (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), (tensor([[-0.0511, -1.4140,  0.1237,  2.1292]], device='cuda:0'), tensor([[-0.0849, -1.0231,  0.1010,  1.5356]], device='cuda:0'), tensor([[-0.0063, -1.0222,  0.0575,  1.5006]], device='cuda:0'), array([-0.1382595 , -1.9322416 ,  0.25909415,  3.111862  ], dtype=float32), tensor([[ 0.0364, -0.2411, -0.0029,  0.3145]], device='cuda:0'), tensor([[ 0.0103, -0.8267,  0.0336,  1.1976]], device='cuda:0'), tensor([[ 0.0229, -0.6313,  0.0156,  0.9001]], device='cuda:0'), tensor([[-0.0165, -0.7674,  0.0240,  1.1438]], device='cuda:0'), tensor([[-0.0267, -1.2180,  0.0875,  1.8107]], device='cuda:0'), tensor([[-0.0229, -1.1483,  0.0774,  1.7875]], device='cuda:0'), tensor([[-0.0793, -1.6101,  0.1663,  2.4575]], device='cuda:0'), tensor([[-0.0318, -0.9629,  0.0469,  1.4439]], device='cuda:0'), tensor([[-0.0050, -0.5722,  0.0070,  0.8489]], device='cuda:0'), tensor([[-0.0038, -0.9527,  0.0478,  1.4803]], device='cuda:0'), tensor([[ 0.0333, -0.1720, -0.0115,  0.3053]], device='cuda:0'), tensor([[-0.0422, -0.2398,  0.0404,  0.2974]], device='cuda:0'), tensor([[-0.0683, -0.8270,  0.0766,  1.2199]], device='cuda:0'), array([-0.15803663, -1.6118481 ,  0.21267907,  2.5282636 ], dtype=float32), tensor([[ 0.0025, -0.3772, -0.0041,  0.5575]], device='cuda:0'), tensor([[-0.0458, -1.3442,  0.1132,  2.1032]], device='cuda:0'), tensor([[-0.0727, -1.5403,  0.1552,  2.4286]], device='cuda:0'), tensor([[-0.0470, -0.4354,  0.0464,  0.6026]], device='cuda:0'), tensor([[ 0.0316, -0.4362,  0.0034,  0.6063]], device='cuda:0'), tensor([[ 0.0062, -0.1822, -0.0095,  0.2678]], device='cuda:0'), tensor([[-0.0557, -0.6312,  0.0584,  0.9095]], device='cuda:0'), tensor([[ 0.0226, -0.5620,  0.0065,  0.8854]], device='cuda:0'), array([-0.11154171, -1.8061751 ,  0.21547833,  2.796214  ], dtype=float32), tensor([[-0.1053, -1.2192,  0.1317,  1.8581]], device='cuda:0'), tensor([[-0.1297, -1.4155,  0.1689,  2.1886]], device='cuda:0'), tensor([[ 0.0113, -0.7572,  0.0242,  1.1801]], device='cuda:0'), tensor([[-0.1035, -1.7364,  0.2038,  2.7646]], device='cuda:0'), tensor([[ 0.0299, -0.3670, -0.0054,  0.5944]], device='cuda:0')), (1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0))\n",
      "len(batch) 4\n",
      "next obs list (tensor([[-0.0511, -1.4140,  0.1237,  2.1292]], device='cuda:0'), tensor([[-0.0849, -1.0231,  0.1010,  1.5356]], device='cuda:0'), tensor([[-0.0063, -1.0222,  0.0575,  1.5006]], device='cuda:0'), array([-0.1382595 , -1.9322416 ,  0.25909415,  3.111862  ], dtype=float32), tensor([[ 0.0364, -0.2411, -0.0029,  0.3145]], device='cuda:0'), tensor([[ 0.0103, -0.8267,  0.0336,  1.1976]], device='cuda:0'), tensor([[ 0.0229, -0.6313,  0.0156,  0.9001]], device='cuda:0'), tensor([[-0.0165, -0.7674,  0.0240,  1.1438]], device='cuda:0'), tensor([[-0.0267, -1.2180,  0.0875,  1.8107]], device='cuda:0'), tensor([[-0.0229, -1.1483,  0.0774,  1.7875]], device='cuda:0'), tensor([[-0.0793, -1.6101,  0.1663,  2.4575]], device='cuda:0'), tensor([[-0.0318, -0.9629,  0.0469,  1.4439]], device='cuda:0'), tensor([[-0.0050, -0.5722,  0.0070,  0.8489]], device='cuda:0'), tensor([[-0.0038, -0.9527,  0.0478,  1.4803]], device='cuda:0'), tensor([[ 0.0333, -0.1720, -0.0115,  0.3053]], device='cuda:0'), tensor([[-0.0422, -0.2398,  0.0404,  0.2974]], device='cuda:0'), tensor([[-0.0683, -0.8270,  0.0766,  1.2199]], device='cuda:0'), array([-0.15803663, -1.6118481 ,  0.21267907,  2.5282636 ], dtype=float32), tensor([[ 0.0025, -0.3772, -0.0041,  0.5575]], device='cuda:0'), tensor([[-0.0458, -1.3442,  0.1132,  2.1032]], device='cuda:0'), tensor([[-0.0727, -1.5403,  0.1552,  2.4286]], device='cuda:0'), tensor([[-0.0470, -0.4354,  0.0464,  0.6026]], device='cuda:0'), tensor([[ 0.0316, -0.4362,  0.0034,  0.6063]], device='cuda:0'), tensor([[ 0.0062, -0.1822, -0.0095,  0.2678]], device='cuda:0'), tensor([[-0.0557, -0.6312,  0.0584,  0.9095]], device='cuda:0'), tensor([[ 0.0226, -0.5620,  0.0065,  0.8854]], device='cuda:0'), array([-0.11154171, -1.8061751 ,  0.21547833,  2.796214  ], dtype=float32), tensor([[-0.1053, -1.2192,  0.1317,  1.8581]], device='cuda:0'), tensor([[-0.1297, -1.4155,  0.1689,  2.1886]], device='cuda:0'), tensor([[ 0.0113, -0.7572,  0.0242,  1.1801]], device='cuda:0'), tensor([[-0.1035, -1.7364,  0.2038,  2.7646]], device='cuda:0'), tensor([[ 0.0299, -0.3670, -0.0054,  0.5944]], device='cuda:0'))\n",
      "tensor([[-0.0511, -1.4140,  0.1237,  2.1292]], device='cuda:0')\n",
      "obs, action tensor([[-0.0267, -1.2180,  0.0875,  1.8107]], device='cuda:0') tensor([[0]], device='cuda:0')\n",
      "tensor([[-0.0849, -1.0231,  0.1010,  1.5356]], device='cuda:0')\n",
      "obs, action tensor([[-0.0683, -0.8270,  0.0766,  1.2199]], device='cuda:0') tensor([[0]], device='cuda:0')\n",
      "tensor([[-0.0063, -1.0222,  0.0575,  1.5006]], device='cuda:0')\n",
      "obs, action tensor([[ 0.0103, -0.8267,  0.0336,  1.1976]], device='cuda:0') tensor([[0]], device='cuda:0')\n",
      "tensor([-0.1383, -1.9322,  0.2591,  3.1119], device='cuda:0')\n",
      "obs, action tensor([[-0.1035, -1.7364,  0.2038,  2.7646]], device='cuda:0') tensor([[0]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-99-9fb11a8de1e2>:128: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(q_values.squeeze(), q_value_targets)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-cc4159693412>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;31m# TODO: Run DQN.optimize() every env_config[\"train_frequency\"] steps.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0menv_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_frequency'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdqn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_dqn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;31m# TODO: Update the target network every env_config[\"target_update_frequency\"] steps.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-9fb11a8de1e2>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(dqn, target_dqn, memory, optimizer)\u001b[0m\n\u001b[1;32m    122\u001b[0m       \u001b[0;31m# Compute the Q-value targets for non-terminal transitions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mnext_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mq_value_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnext_q_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "## train.py\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "# import config\n",
    "# from utils import preprocess\n",
    "# from evaluate import evaluate_policy\n",
    "# from dqn import DQN, ReplayMemory, optimize\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--env', choices=['CartPole-v1'], default='CartPole-v1')\n",
    "# parser.add_argument('--evaluate_freq', type=int, default=25, help='How often to run evaluation.', nargs='?')\n",
    "# parser.add_argument('--evaluation_episodes', type=int, default=5, help='Number of evaluation episodes.', nargs='?')\n",
    "args = argparse.Namespace(\n",
    "    env='CartPole-v1',\n",
    "    evaluate_freq=2,\n",
    "    evaluation_episodes=1000\n",
    ")\n",
    "\n",
    "# Hyperparameter configurations for different environments. See config.py.\n",
    "ENV_CONFIGS = {\n",
    "    'CartPole-v1': CartPole\n",
    "}\n",
    "\n",
    "def evaluate_policy(dqn, env, env_config, args, n_episodes, render=False, verbose=False):\n",
    "    \"\"\"Runs {n_episodes} episodes to evaluate current policy.\"\"\"\n",
    "    total_return = 0\n",
    "    for i in range(n_episodes):\n",
    "        obs, info = env.reset()\n",
    "        obs = preprocess(obs, env=args.env).unsqueeze(0)\n",
    "\n",
    "        terminated = False\n",
    "        episode_return = 0\n",
    "\n",
    "        while not terminated:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            action = dqn.act(obs, exploit=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            obs = preprocess(obs, env=args.env).unsqueeze(0)\n",
    "\n",
    "            episode_return += reward\n",
    "\n",
    "        total_return += episode_return\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Finished episode {i+1} with a total return of {episode_return}')\n",
    "\n",
    "\n",
    "    return total_return / n_episodes\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    # Initialize environment and config.\n",
    "    env = gym.make(args.env)\n",
    "    env_config = ENV_CONFIGS[args.env]\n",
    "\n",
    "    # Initialize deep Q-networks.\n",
    "    dqn = DQN(env_config=env_config).to(device)\n",
    "    # TODO: Create and initialize target Q-network.\n",
    "    target_dqn = DQN(env_config=env_config).to(device)  # Initialize target Q-network.\n",
    "    target_dqn.load_state_dict(dqn.state_dict())  # Initialize target Q-network with DQN weights.\n",
    "    target_dqn.eval()\n",
    "\n",
    "    # Create replay memory.\n",
    "    memory = ReplayMemory(env_config['memory_size'])\n",
    "\n",
    "    # Initialize optimizer used for training the DQN. We use Adam rather than RMSProp.\n",
    "    optimizer = torch.optim.Adam(dqn.parameters(), lr=env_config['lr'])\n",
    "\n",
    "    # Keep track of best evaluation mean return achieved so far.\n",
    "    best_mean_return = -float(\"Inf\")\n",
    "\n",
    "    for episode in range(env_config['n_episodes']):\n",
    "        terminated = False\n",
    "        obs, info = env.reset()\n",
    "\n",
    "        obs = preprocess(obs, env=args.env).unsqueeze(0)\n",
    "        step = 1\n",
    "        while not terminated:\n",
    "\n",
    "            # TODO: Get action from DQN.\n",
    "            # print(dqn.act(obs, exploit=True))\n",
    "            action = dqn.act(obs, exploit=True)\n",
    "\n",
    "            # Act in the true environment.\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # Preprocess incoming observation.\n",
    "            if not terminated:\n",
    "                next_obs = preprocess(next_obs, env=args.env).unsqueeze(0)\n",
    "\n",
    "            # TODO: Add the transition to the replay memory. Remember to convert\n",
    "            #       everything to PyTorch tensors!\n",
    "\n",
    "\n",
    "            memory.push(obs, action, next_obs, reward)\n",
    "\n",
    "            # TODO: Run DQN.optimize() every env_config[\"train_frequency\"] steps.\n",
    "            if step % env_config['train_frequency'] == 0:\n",
    "                optimize(dqn, target_dqn, memory, optimizer)\n",
    "\n",
    "            # TODO: Update the target network every env_config[\"target_update_frequency\"] steps.\n",
    "            if step % env_config['target_update_frequency'] == 0:\n",
    "                target_dqn.load_state_dict(dqn.state_dict())\n",
    "            obs = next_obs\n",
    "            step += 1\n",
    "\n",
    "\n",
    "        # Evaluate the current agent.\n",
    "        if episode % args.evaluate_freq == 0:\n",
    "            mean_return = evaluate_policy(dqn, env, env_config, args, n_episodes=args.evaluation_episodes)\n",
    "            print(f'Episode {episode+1}/{env_config[\"n_episodes\"]}: {mean_return}')\n",
    "\n",
    "            # Save current agent if it has the best performance so far.\n",
    "            if mean_return >= best_mean_return:\n",
    "                best_mean_return = mean_return\n",
    "\n",
    "                print('Best performance so far! Saving model.')\n",
    "                model_path = f'models/{args.env}_best.pt'\n",
    "\n",
    "                # Check if the directory exists, if not, create it\n",
    "                if not os.path.exists(os.path.dirname(model_path)):\n",
    "                    os.makedirs(os.path.dirname(model_path))\n",
    "\n",
    "                # Now you can safely save your model\n",
    "                torch.save(dqn, model_path)\n",
    "                # torch.save(dqn, f'models/{args.env}_best.pt')\n",
    "\n",
    "    # Close environment after training is completed.\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "W3S4ixtx0jYa"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "fFx3f2G9e5x8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M2GGB6yiilpv",
    "outputId": "1b809081-9e94-434d-eeb2-1fde5fa6a163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The policy got a mean return of 9.636 over 1000 episodes.\n",
      "The policy got a mean return of 9.636 over 1000 episodes.\n"
     ]
    }
   ],
   "source": [
    "## Evaluate.py\n",
    "import argparse\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "\n",
    "# import config\n",
    "# from utils import preprocess\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--env', choices=['CartPole-v1'], default='CartPole-v1')\n",
    "# parser.add_argument('--path', type=str, help='Path to stored DQN model.')\n",
    "# parser.add_argument('--n_eval_episodes', type=int, default=1, help='Number of evaluation episodes.', nargs='?')\n",
    "# parser.add_argument('--render', dest='render', action='store_true', help='Render the environment.')\n",
    "# parser.add_argument('--save_video', dest='save_video', action='store_true', help='Save the episodes as video.')\n",
    "# parser.set_defaults(render=False)\n",
    "# parser.set_defaults(save_video=False)\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    # env='CartPole-v1',\n",
    "    # evaluate_freq=25,\n",
    "    # evaluation_episodes=5\n",
    "    env = 'CartPole-v1',\n",
    "    path = 'models/CartPole-v1_best.pt',\n",
    "    n_eval_episodes = 1000,\n",
    "    render = False,\n",
    "    save_video = False\n",
    ")\n",
    "\n",
    "# Hyperparameter configurations for different environments. See config.py.\n",
    "ENV_CONFIGS = {\n",
    "    'CartPole-v1': CartPole,\n",
    "}\n",
    "\n",
    "\n",
    "# def evaluate_policy(dqn, env, env_config, args, n_episodes, render=False, verbose=False):\n",
    "#     \"\"\"Runs {n_episodes} episodes to evaluate current policy.\"\"\"\n",
    "#     total_return = 0\n",
    "#     for i in range(n_episodes):\n",
    "#         obs, info = env.reset()\n",
    "#         obs = preprocess(obs, env=args.env).unsqueeze(0)\n",
    "\n",
    "#         terminated = False\n",
    "#         episode_return = 0\n",
    "\n",
    "#         while not terminated:\n",
    "#             if render:\n",
    "#                 env.render()\n",
    "\n",
    "#             action = dqn.act(obs, exploit=True).item()\n",
    "#             obs, reward, terminated, truncated, info = env.step(action)\n",
    "#             obs = preprocess(obs, env=args.env).unsqueeze(0)\n",
    "\n",
    "#             episode_return += reward\n",
    "\n",
    "#         total_return += episode_return\n",
    "\n",
    "#         if verbose:\n",
    "#             print(f'Finished episode {i+1} with a total return of {episode_return}')\n",
    "\n",
    "\n",
    "#     return total_return / n_episodes\n",
    "def evaluate_policy(dqn, env, env_config, args, n_eval_episodes, render=False, verbose=True):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dqn.to(device)  # Move the model to the correct device\n",
    "\n",
    "    total_rewards = []\n",
    "    for _ in range(n_eval_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)  # Convert state to tensor and move to device\n",
    "\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action = dqn.act(state)  # Assuming dqn.act returns an action based on the current state\n",
    "            # print(env.step(action))\n",
    "            next_state, reward, done, additional_flag, info = env.step(action)\n",
    "            next_state = torch.from_numpy(next_state).float().unsqueeze(0).to(device)  # Convert next_state to tensor and move to device\n",
    "\n",
    "            # Store the transition in memory (if applicable)\n",
    "            #...\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "\n",
    "    mean_return = np.mean(total_rewards)\n",
    "    if verbose:\n",
    "        print(f'The policy got a mean return of {mean_return} over {n_eval_episodes} episodes.')\n",
    "\n",
    "    return mean_return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # args = parser.parse_args()\n",
    "\n",
    "    # Initialize environment and config\n",
    "    env = gym.make(args.env)\n",
    "    env_config = ENV_CONFIGS[args.env]\n",
    "\n",
    "    if args.save_video:\n",
    "        env = gym.make(args.env, render_mode='rgb_array')\n",
    "        env = gym.wrappers.RecordVideo(env, './video/', episode_trigger=lambda episode_id: True)\n",
    "\n",
    "    # Load model from provided path.\n",
    "    dqn = torch.load(args.path, map_location=torch.device('cpu'))\n",
    "    dqn.eval()\n",
    "\n",
    "    mean_return = evaluate_policy(dqn, env, env_config, args, args.n_eval_episodes, render=args.render and not args.save_video, verbose=True)\n",
    "    print(f'The policy got a mean return of {mean_return} over {args.n_eval_episodes} episodes.')\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_1FCgpUke55d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "UfjOowY_e5_7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GBbK66mfe6Gr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
