{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "BVJNKmvUGYi4",
        "outputId": "03314013-4038-46b5-9417-01e3c684be94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/1000: 9.333\n",
            "Episode 26/1000: 9.351\n",
            "Episode 51/1000: 9.353\n",
            "Episode 76/1000: 36.068\n",
            "Episode 101/1000: 147.133\n",
            "Episode 126/1000: 98.091\n",
            "Episode 151/1000: 492.529\n",
            "Episode 176/1000: 441.993\n",
            "Episode 201/1000: 470.483\n",
            "Episode 226/1000: 353.619\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-63bd2bc4f23c>\u001b[0m in \u001b[0;36m<cell line: 201>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0menv_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_frequency\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m           \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-63bd2bc4f23c>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, state, action, reward, next_state, done, batch_size, step)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import gym\n",
        "import torch.nn.functional as F\n",
        "import argparse\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "    def push(self, obs, action, next_obs, reward, donne):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "\n",
        "        self.memory[self.position] = (obs, action, reward, next_obs, donne)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Samples batch_size transitions from the replay memory and returns a tuple\n",
        "            (obs, action, next_obs, reward)\n",
        "        \"\"\"\n",
        "        sample = random.sample(self.memory, batch_size)\n",
        "        return zip(*sample)\n",
        "\n",
        "# Initialize environment and agent\n",
        "env = gym.make('CartPole-v1')\n",
        "# env = gym.wrappers.RecordVideo(env, './video/', episode_trigger=lambda episode_id: True)\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "CartPole = {\n",
        "    'memory_size': 50000,\n",
        "    'n_episodes': 1000,\n",
        "    'batch_size': 32,\n",
        "    'target_update_frequency': 100,\n",
        "    'train_frequency': 1,\n",
        "    'gamma': 0.95,\n",
        "    'lr': 1e-4,\n",
        "    'eps_start': 1.0,\n",
        "    'eps_end': 0.05,\n",
        "    'anneal_length': 10**4,\n",
        "    'n_actions': 2,\n",
        "    'epsilon': 0.9\n",
        "}\n",
        "ENV_CONFIGS = {\n",
        "    'CartPole-v1': CartPole\n",
        "}\n",
        "\n",
        "env_config = ENV_CONFIGS['CartPole-v1']\n",
        "replay_memory = ReplayMemory(env_config['memory_size'])\n",
        "\n",
        "\n",
        "# Define the Q-network\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size, hidden_size=256):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "    def forward(self, x):\n",
        "        # print('x: ',x.shape)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "q_network = QNetwork(state_size, action_size).to(device)\n",
        "target_network = QNetwork(state_size, action_size).to(device)\n",
        "target_network.load_state_dict(q_network.state_dict())\n",
        "target_network.eval()\n",
        "\n",
        "# Define the DQN agent\n",
        "class DQN:\n",
        "    def __init__(self, state_size, action_size, lr=0.001, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        #self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        #self.q_network = QNetwork(state_size, action_size).to(self.device)\n",
        "        #self.target_network = QNetwork(state_size, action_size).to(self.device)\n",
        "        #self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        #self.target_network.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
        "        self.loss_function = F.mse_loss\n",
        "\n",
        "    def act(self, state, exploit = False):\n",
        "       if exploit == False:\n",
        "          if random.random() > self.epsilon:\n",
        "              with torch.no_grad():\n",
        "                  state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "                  q_values = q_network(state)\n",
        "                  action = q_values.argmax().item()\n",
        "          else:\n",
        "              action = random.randrange(self.action_size)\n",
        "          return action\n",
        "       else:\n",
        "         with torch.no_grad():\n",
        "                  state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "                  q_values = q_network(state)\n",
        "                  action = q_values.argmax().item()\n",
        "                  return action\n",
        "\n",
        "\n",
        "    def optimize(self, state, action, reward, next_state, done, batch_size=32, step = 1):\n",
        "        if len(replay_memory) < batch_size:\n",
        "            return\n",
        "\n",
        "        states, actions, next_states, rewards, dones = replay_memory.sample(batch_size)\n",
        "\n",
        "        states = torch.FloatTensor(states).to(device)\n",
        "        actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
        "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
        "        next_states = torch.FloatTensor(next_states).to(device)\n",
        "        dones = torch.FloatTensor(dones).unsqueeze(1).to(device)\n",
        "        # print(\"states:\", states.shape)\n",
        "        # print(\"actions:\", actions.shape)\n",
        "        # print(\"rewards:\", rewards.shape)\n",
        "        # print(\"next_states:\", next_states.shape)\n",
        "        # print(\"dones:\", dones.shape)\n",
        "\n",
        "\n",
        "        q_values = q_network(states).gather(1, actions)\n",
        "        next_q_values = target_network(next_states).max(dim=1, keepdim=True)[0].detach()\n",
        "        target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
        "\n",
        "        loss = self.loss_function(q_values, target_q_values)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update target network\n",
        "        if step % env_config['target_update_frequency']:\n",
        "            target_network.load_state_dict(q_network.state_dict())\n",
        "\n",
        "        # Update epsilon\n",
        "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "\n",
        "\n",
        "agent = DQN(state_size, action_size)\n",
        "# Training parameters\n",
        "# n_episodes = 1000\n",
        "batch_size = 32\n",
        "\n",
        "args = argparse.Namespace(\n",
        "    env='CartPole-v1',\n",
        "    evaluate_freq=25,\n",
        "    evaluation_episodes=1000\n",
        ")\n",
        "\n",
        "def evaluate_policy(agent, env, env_config, args, n_episodes, render=False, verbose=False):\n",
        "    \"\"\"Runs {n_episodes} episodes to evaluate current policy.\"\"\"\n",
        "    total_return = 0\n",
        "    for i in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        #state = preprocess(obs, env=args.env).unsqueeze(0)\n",
        "\n",
        "        terminated = False\n",
        "        episode_return = 0\n",
        "        truncated = False\n",
        "        while not terminated and not truncated:\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "            action = agent.act(state, exploit=True)\n",
        "            state, reward, terminated, truncated = env.step(action)\n",
        "            #obs = preprocess(obs, env=args.env).unsqueeze(0)\n",
        "\n",
        "            episode_return += reward\n",
        "\n",
        "        total_return += episode_return\n",
        "\n",
        "        if verbose:\n",
        "            print(f'Finished episode {i+1} with a total return of {episode_return}')\n",
        "\n",
        "\n",
        "    return total_return / n_episodes\n",
        "\n",
        "best_mean_return = -float(\"Inf\")\n",
        "for episode in range(env_config['n_episodes']):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    step = 1\n",
        "    while True:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        replay_memory.push(state, action, reward, next_state, done)\n",
        "        total_reward += reward\n",
        "        if step%env_config[\"train_frequency\"] == 0:\n",
        "          agent.optimize(state, action, reward, next_state, done, batch_size, step)\n",
        "        step += 1\n",
        "        state = next_state\n",
        "\n",
        "        if done or _:\n",
        "            break\n",
        "    env.close()\n",
        "\n",
        "    # print(f\"Episode: {episode + 1}, Total Train Reward: {total_reward}\")\n",
        "    if episode % args.evaluate_freq == 0:\n",
        "      mean_return = evaluate_policy(agent, env, env_config, args, args.evaluation_episodes)\n",
        "      print(f'Episode {episode+1}/{env_config[\"n_episodes\"]}: {mean_return}')\n",
        "      # # Save current agent if it has the best performance so far.\n",
        "      # if mean_return >= best_mean_return:\n",
        "      #     best_mean_return = mean_return\n",
        "\n",
        "      #     # print('Best performance so far! Saving model.')\n",
        "      #     # torch.save(dqn, f'models/{args.env}_best.pt')\n",
        "      #     print('Best performance so far! Saving model.')\n",
        "      #     model_path = f'models/{args.env}_best.pt'\n",
        "\n",
        "      #     # Check if the directory exists, if not, create it\n",
        "      #     if not os.path.exists(os.path.dirname(model_path)):\n",
        "      #         os.makedirs(os.path.dirname(model_path))\n",
        "\n",
        "      #     # Now you can safely save your model\n",
        "      #     torch.save(agent, model_path)\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OWd-q45XHfl3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}