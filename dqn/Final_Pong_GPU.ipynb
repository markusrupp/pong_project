{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install -U gymnasium\n",
        "%pip install -U gymnasium[atari,accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHhKbkOrUAlt",
        "outputId": "b1567ccb-982f-4fb3-9964-41375d05f630"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (4.66.4)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari]) (6.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2024.2.2)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446659 sha256=a9ac9d05696f90fcb4bfd1dca17e09b6db50ecdc4bae390c8e5ae1458a7be2f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: ale-py, shimmy, AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 shimmy-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import gymnasium as gym"
      ],
      "metadata": {
        "id": "TAYv6-0uUFOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import gym\n",
        "import torch.nn.functional as F\n",
        "import argparse"
      ],
      "metadata": {
        "id": "AFc3v8TJUHO3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('ALE/Pong-v5')"
      ],
      "metadata": {
        "id": "7DfdzoRDUPDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "754b0bbe-9560-4587-e5e9-3f284546cc82"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.wrappers import AtariPreprocessing"
      ],
      "metadata": {
        "id": "akbwlvUWYdB0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# env = gym.make('ALE/Pong-v5')\n",
        "# env = AtariPreprocessing(env, screen_size=84, grayscale_obs=True, frame_skip=1, noop_max=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycFTo1W_YntT",
        "outputId": "2d29d5cc-a12a-4ada-9040-f352aeb00bc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Preprocessing Frames and Stacking Observations\n",
        "# class FrameStackingWrapper(gym.Wrapper):\n",
        "#     def __init__(self, env, obs_stack_size):\n",
        "#         super(FrameStackingWrapper, self).__init__(env)\n",
        "#         self.obs_stack_size = obs_stack_size\n",
        "#         self.obs_stack = None\n",
        "\n",
        "#     def reset(self, **kwargs):\n",
        "#         obs = self.env.reset(**kwargs)\n",
        "#         self.obs_stack = torch.cat([obs.unsqueeze(0)] * self.obs_stack_size, dim=0)\n",
        "#         return self.obs_stack\n",
        "\n",
        "#     def step(self, action):\n",
        "#         next_obs, reward, done, info = self.env.step(action)\n",
        "#         self.obs_stack = torch.cat((self.obs_stack[1:, ...], next_obs.unsqueeze(0)), dim=0)\n",
        "#         return self.obs_stack, reward, done, info\n",
        "\n",
        "# env = FrameStackingWrapper(env, obs_stack_size=4)  # Assuming 4 frames for frame stacking\n",
        "# state_size = env.observation_space.shape[0]\n",
        "# print(\"state size\", state_size)"
      ],
      "metadata": {
        "id": "NxnXX4QgYyHI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Install necessary dependencies\n",
        "# !apt-get install -y xvfb x11-utils\n",
        "# !pip install gym[atari]\n",
        "# !pip install pyvirtualdisplay"
      ],
      "metadata": {
        "id": "9Ztzw6VQkIqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create a virtual display\n",
        "# from pyvirtualdisplay import Display\n",
        "# display = Display(visible=0, size=(1400, 900))\n",
        "# display.start()"
      ],
      "metadata": {
        "id": "fJzDcZsxklet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgXnzljoTBRN",
        "outputId": "f67fbe20-8ee7-41d0-fab5-d9300f7098ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:137: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting a numpy array, actual type: <class 'tuple'>\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/spaces/box.py:226: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
            "  logger.warn(\"Casting input x to numpy array.\")\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:167: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space with exception: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\u001b[0m\n",
            "  logger.warn(f\"{pre} is not within the observation space with exception: {e}\")\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "<ipython-input-14-5f0e359163c7>:157: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  states = torch.tensor(states, dtype=torch.float32, device=device)\n",
            "<ipython-input-14-5f0e359163c7>:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  next_states = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
            "<ipython-input-14-5f0e359163c7>:176: UserWarning: Using a target size (torch.Size([32, 32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  loss = self.loss_function(q_values, target_q_values)\n",
            "<ipython-input-14-5f0e359163c7>:128: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  state = torch.tensor(state, dtype=torch.float32, device=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1, Total Reward: -21.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-5f0e359163c7>:137: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  state = torch.tensor(state, dtype=torch.float32, device=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "Episode: 2, Total Reward: -21.0\n",
            "Episode: 3, Total Reward: -21.0\n",
            "Episode: 4, Total Reward: -21.0\n",
            "Episode: 5, Total Reward: -21.0\n",
            "Episode: 6, Total Reward: -21.0\n",
            "Episode: 7, Total Reward: -20.0\n",
            "Episode: 8, Total Reward: -19.0\n",
            "Episode: 9, Total Reward: -21.0\n",
            "Episode: 10, Total Reward: -21.0\n",
            "Episode: 11, Total Reward: -21.0\n",
            "Episode: 12, Total Reward: -20.0\n",
            "Episode: 13, Total Reward: -21.0\n",
            "Episode: 14, Total Reward: -21.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "    def push(self, obs, action, next_obs, reward, donne):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "\n",
        "        self.memory[self.position] = (obs, action, reward, next_obs, donne)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Samples batch_size transitions from the replay memory and returns a tuple\n",
        "            (obs, action, next_obs, reward)\n",
        "        \"\"\"\n",
        "        sample = random.sample(self.memory, batch_size)\n",
        "        return zip(*sample)\n",
        "\n",
        "# Initialize environment and agent\n",
        "# env = gym.make('CartPole-v1')\n",
        "env = gym.make('ALE/Pong-v5')\n",
        "env = AtariPreprocessing(env, screen_size=84, grayscale_obs=True, frame_skip=1, noop_max=30)\n",
        "\n",
        "# state_size = env.observation_space.shape\n",
        "# print(\"state size\", state_size)\n",
        "# env = gym.wrappers.RecordVideo(env, './video/', episode_trigger=lambda episode_id: True)\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "CartPole = {\n",
        "    'Observation_stack_size': 4,\n",
        "    'memory_size': 10000,\n",
        "    'n_episodes': 100,\n",
        "    'batch_size': 32,\n",
        "    'target_update_frequency': 100,\n",
        "    'train_frequency': 1,\n",
        "    'gamma': 0.95,\n",
        "    'lr': 1e-4,\n",
        "    'eps_start': 1.0,\n",
        "    'eps_end': 0.05,\n",
        "    'anneal_length': 10**6,\n",
        "    'n_actions': 2,\n",
        "    'epsilon': 0.9\n",
        "}\n",
        "ENV_CONFIGS = {\n",
        "    'CartPole-v1': CartPole\n",
        "}\n",
        "\n",
        "env_config = ENV_CONFIGS['CartPole-v1']\n",
        "replay_memory = ReplayMemory(env_config['memory_size'])\n",
        "\n",
        "\n",
        "# Define the Q-network\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        # self.fc1 = nn.Linear(state_size, hidden_size)\n",
        "        # self.fc2 = nn.Linear(hidden_size, action_size)\n",
        "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
        "        self.fc2 = nn.Linear(512, action_size)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "    def forward(self, x):\n",
        "        # x = torch.cat([x] * 4, dim=0)\n",
        "        x = x.squeeze(1)\n",
        "\n",
        "        # x = x[:4, :, :]\n",
        "\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.relu(self.conv3(x))\n",
        "        # print('x: ',x.shape)\n",
        "        x = self.flatten(x)\n",
        "        # print('x flatten: ',x.shape)\n",
        "        # x = x.reshape(1, 3136)\n",
        "\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        # print('x reshape: ',x.shape)\n",
        "        return x\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "q_network = QNetwork(state_size, action_size).to(device)\n",
        "target_network = QNetwork(state_size, action_size).to(device)\n",
        "target_network.load_state_dict(q_network.state_dict())\n",
        "target_network.eval()\n",
        "\n",
        "\n",
        "\n",
        "# Define the DQN agent\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, lr=0.0001, gamma=0.95, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        #self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        #self.q_network = QNetwork(state_size, action_size).to(self.device)\n",
        "        #self.target_network = QNetwork(state_size, action_size).to(self.device)\n",
        "        #self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        #self.target_network.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
        "        self.loss_function = F.mse_loss\n",
        "\n",
        "    def act(self, state, exploit = False):\n",
        "       if exploit == False:\n",
        "          if random.random() > self.epsilon:\n",
        "              with torch.no_grad():\n",
        "                  # state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "                  state = torch.tensor(state, dtype=torch.float32, device=device)\n",
        "                  q_values = q_network(state)\n",
        "                  action = q_values.argmax().item()\n",
        "          else:\n",
        "              action = random.randrange(self.action_size)\n",
        "          return action\n",
        "       else:\n",
        "         with torch.no_grad():\n",
        "                  # state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "                  state = torch.tensor(state, dtype=torch.float32, device=device)\n",
        "                  q_values = q_network(state)\n",
        "                  action = q_values.argmax().item()\n",
        "                  return action\n",
        "\n",
        "\n",
        "    def optimize(self, state, action, reward, next_state, done, batch_size=32, step = 1):\n",
        "        if len(replay_memory) < batch_size:\n",
        "            return\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        # batch = random.sample(replay_memory, batch_size)\n",
        "        # states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        states, actions, next_states, rewards, dones = replay_memory.sample(batch_size)\n",
        "        # print('states: ',states)\n",
        "        states = torch.stack(states).squeeze(1).to(device)\n",
        "        # print('states stack size: ',states.shape)\n",
        "        next_states = torch.stack(next_states).squeeze(1).squeeze(1)\n",
        "        # print(\"states\",states.shape)\n",
        "        # states = torch.FloatTensor(states).to(device)\n",
        "        states = torch.tensor(states, dtype=torch.float32, device=device)\n",
        "        actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n",
        "        # rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
        "        # next_states = torch.FloatTensor(next_states).to(device)\n",
        "        next_states = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
        "        # dones = torch.FloatTensor(dones).unsqueeze(1).to(device)\n",
        "        dones = torch.tensor(dones, dtype=torch.float32, device=device)\n",
        "        # print(\"states:\", states.shape)\n",
        "        # print(\"actions:\", actions.shape)\n",
        "        # print(\"rewards:\", rewards.shape)\n",
        "        # print(\"next_states:\", next_states.shape)\n",
        "        # print(\"dones:\", dones.shape)\n",
        "\n",
        "        # print(\"q network: \", q_network(states))\n",
        "        q_values = q_network(states).gather(1, actions)\n",
        "        next_q_values = target_network(next_states).max(dim=1, keepdim=True)[0].detach()\n",
        "        target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
        "\n",
        "        loss = self.loss_function(q_values, target_q_values)\n",
        "        # print(\"loss: \", loss)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update target network\n",
        "        if step % env_config['target_update_frequency']:\n",
        "            target_network.load_state_dict(q_network.state_dict())\n",
        "\n",
        "        # Update epsilon\n",
        "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "\n",
        "\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "# Training parameters\n",
        "# n_episodes = 25\n",
        "batch_size = 32\n",
        "\n",
        "args = argparse.Namespace(\n",
        "    env='ALE/Pong-v5',\n",
        "    evaluate_freq=25,\n",
        "    evaluation_episodes=1000\n",
        ")\n",
        "\n",
        "def evaluate_policy(agent, env, env_config, args, n_episodes, render=False, verbose=False):\n",
        "    \"\"\"Runs {n_episodes} episodes to evaluate current policy.\"\"\"\n",
        "    total_return = 0\n",
        "    for i in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        obs_stack = torch.cat(env_config['Observation_stack_size'] * [state]).unsqueeze(0).to(device)\n",
        "        #state = preprocess(obs, env=args.env).unsqueeze(0)\n",
        "        # print(\"state:\", state)\n",
        "        # print(\"state shape:\", state.shape)\n",
        "\n",
        "        terminated = False\n",
        "        episode_return = 0\n",
        "        truncated = False\n",
        "        while not terminated and not truncated:\n",
        "            if render:\n",
        "                env.render()\n",
        "\n",
        "            action = agent.act(obs_stack, exploit=True)\n",
        "            next_state, reward, terminated, truncated = env.step(action)\n",
        "            #obs = preprocess(obs, env=args.env).unsqueeze(0)\n",
        "            obs_stack = obs_stack.to(device)\n",
        "            next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            next_state = torch.cat((obs_stack[:, 1:, ...], next_state.unsqueeze(1)), dim=1).to(device)\n",
        "            # print('next_state')\n",
        "            # state = obs_stack\n",
        "\n",
        "\n",
        "            episode_return += reward\n",
        "\n",
        "        total_return += episode_return\n",
        "\n",
        "        if verbose:\n",
        "            print(f'Finished episode {i+1} with a total return of {episode_return}')\n",
        "\n",
        "\n",
        "    return total_return / n_episodes\n",
        "\n",
        "# print('ob stac size',env_config['Observation_stack_size'])\n",
        "for episode in range(env_config['n_episodes']):\n",
        "    state = env.reset()\n",
        "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "    # print('obs stack', state.shape)\n",
        "    total_reward = 0\n",
        "    step = 1\n",
        "    # obs_stack_size = torch.stack(state).shape\n",
        "    obs_stack = torch.cat(env_config['Observation_stack_size'] * [state]).unsqueeze(0).to(device)\n",
        "    # print('obs stack lkk', obs_stack[:, 1:, ...])\n",
        "    while True:\n",
        "        action = agent.act(obs_stack)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        # print(done, truncated)\n",
        "\n",
        "        next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
        "        next_state = next_state.to(device)\n",
        "        obs_stack = obs_stack.to(device)\n",
        "        next_state = torch.cat((obs_stack[:, 1:, ...], next_state.unsqueeze(1)), dim=1).to(device)\n",
        "        # print('next_state')\n",
        "        state = obs_stack\n",
        "        # print('obs_state', state.shape)\n",
        "\n",
        "        replay_memory.push(state, action, reward, next_state, done)\n",
        "        # print(\"reward: \", reward)\n",
        "        # print(\"states:\", state.shape)#[1,4,84,84]\n",
        "        # print(\"actions:\", action)\n",
        "        # print(\"rewards:\", reward)\n",
        "        # print(\"next_states:\", next_state.shape) #[1,4,84,84]\n",
        "        # print(\"dones:\", done)\n",
        "\n",
        "\n",
        "        if step%env_config[\"train_frequency\"] == 0:\n",
        "          # print('step',step)\n",
        "          agent.optimize(state, action, reward, next_state, done, batch_size, step)\n",
        "        total_reward += reward\n",
        "        step += 1\n",
        "        # print('step',step)\n",
        "        state = next_state\n",
        "         # Render the environment\n",
        "        # env.render()\n",
        "        if done:\n",
        "            break\n",
        "    # env.close()\n",
        "\n",
        "    print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
        "    if episode % args.evaluation_episodes == 0:\n",
        "      print(evaluate_policy(agent, env, env_config, args, args.evaluation_episodes))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v3xA9LMzWOYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "634D4sMdouoN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}