{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ## all packages\n",
        "# ! pip install gymnasium[all]\n",
        "# !apt-get install git\n",
        "# !git clone https://github.com/magni84/gym_RLcourse.git\n",
        "# %cd gym_RLcourse\n",
        "# !pip install -e .\n",
        "# !pip install matplotlib\n",
        "# !pip install pygame\n",
        "\n",
        "# ! pip install gymnasium 'gymnasium[atari]' 'gymnasium[accept-rom-license]'\n",
        "\n",
        "# import gymnasium as gym\n",
        "# import gym_RLcourse\n",
        "# import numpy as np\n",
        "# import matplotlib\n",
        "# import matplotlib.pyplot as plt\n",
        "# from IPython.display import clear_output # Used to clear the ouput of a Jupyter cell.\n",
        "\n",
        "# from collections import namedtuple, deque\n",
        "# from itertools import count\n",
        "# import math\n",
        "# import random\n",
        "# ### Torch imports\n",
        "# import torch\n",
        "# # from torch import nn\n",
        "# from torch.utils.data import DataLoader\n",
        "# from torchvision import datasets\n",
        "# from torchvision.transforms import ToTensor\n",
        "\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import argparse"
      ],
      "metadata": {
        "id": "8sb8w10uSAFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "    def push(self, obs, action, next_obs, reward, donne):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "\n",
        "        self.memory[self.position] = (obs, action, reward, next_obs, donne)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Samples batch_size transitions from the replay memory and returns a tuple\n",
        "            (obs, action, next_obs, reward)\n",
        "        \"\"\"\n",
        "        sample = random.sample(self.memory, batch_size)\n",
        "        return zip(*sample)\n",
        "\n",
        "\n",
        "# Define the Q-network\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size, hidden_size=256):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "    def forward(self, x):\n",
        "        # print('x: ',x.shape)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the DQN agent\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, lr=0.001, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.q_network = QNetwork(state_size, action_size).to(self.device)\n",
        "        self.target_network = QNetwork(state_size, action_size).to(self.device)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        self.target_network.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
        "        self.loss_function = nn.MSELoss()\n",
        "\n",
        "    def act(self, state):\n",
        "        if random.random() > self.epsilon:\n",
        "            with torch.no_grad():\n",
        "                state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "                q_values = self.q_network(state)\n",
        "                action = q_values.argmax().item()\n",
        "        else:\n",
        "            action = random.randrange(self.action_size)\n",
        "        return action\n",
        "\n",
        "    def train(self, state, action, reward, next_state, done, batch_size=32):\n",
        "        if len(replay_memory) < batch_size:\n",
        "            return\n",
        "\n",
        "        # batch = random.sample(replay_memory, batch_size)\n",
        "        # states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        states, actions, next_states, rewards, dones = replay_memory.sample(batch_size)\n",
        "\n",
        "        states = torch.FloatTensor(states).to(self.device)\n",
        "        actions = torch.LongTensor(actions).unsqueeze(1).to(self.device)\n",
        "        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)\n",
        "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
        "        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)\n",
        "        # print(\"states:\", states.shape)\n",
        "        # print(\"actions:\", actions.shape)\n",
        "        # print(\"rewards:\", rewards.shape)\n",
        "        # print(\"next_states:\", next_states.shape)\n",
        "        # print(\"dones:\", dones.shape)\n",
        "\n",
        "\n",
        "        q_values = self.q_network(states).gather(1, actions)\n",
        "        next_q_values = self.target_network(next_states).max(dim=1, keepdim=True)[0].detach()\n",
        "        target_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
        "\n",
        "        loss = self.loss_function(q_values, target_q_values)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update target network\n",
        "        if done:\n",
        "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "        # Update epsilon\n",
        "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "# Initialize environment and agent\n",
        "env = gym.make('CartPole-v1')\n",
        "# env = gym.wrappers.RecordVideo(env, './video/', episode_trigger=lambda episode_id: True)\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "agent = DQNAgent(state_size, action_size)\n",
        "\n",
        "# Initialize replay memory\n",
        "replay_memory = []\n",
        "\n",
        "CartPole = {\n",
        "    'memory_size': 50000,\n",
        "    'n_episodes': 1000,\n",
        "    'batch_size': 32,\n",
        "    'target_update_frequency': 100,\n",
        "    'train_frequency': 1,\n",
        "    'gamma': 0.95,\n",
        "    'lr': 1e-4,\n",
        "    'eps_start': 1.0,\n",
        "    'eps_end': 0.05,\n",
        "    'anneal_length': 10**4,\n",
        "    'n_actions': 2,\n",
        "    'epsilon': 0.9\n",
        "}\n",
        "ENV_CONFIGS = {\n",
        "    'CartPole-v1': CartPole\n",
        "}\n",
        "\n",
        "env_config = ENV_CONFIGS['CartPole-v1']\n",
        "replay_memory = ReplayMemory(env_config['memory_size'])\n",
        "\n",
        "\n",
        "# Training parameters\n",
        "n_episodes = 1000\n",
        "batch_size = 32\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "\n",
        "    while True:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        replay_memory.push(state, action, reward, next_state, done)\n",
        "        total_reward += reward\n",
        "\n",
        "        agent.train(state, action, reward, next_state, done, batch_size)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "58lAzRq9RcZV",
        "outputId": "cdcc73ff-133f-4a79-b4c4-2396bb6e983e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1, Total Reward: 44.0\n",
            "Episode: 2, Total Reward: 74.0\n",
            "Episode: 3, Total Reward: 25.0\n",
            "Episode: 4, Total Reward: 11.0\n",
            "Episode: 5, Total Reward: 13.0\n",
            "Episode: 6, Total Reward: 11.0\n",
            "Episode: 7, Total Reward: 9.0\n",
            "Episode: 8, Total Reward: 13.0\n",
            "Episode: 9, Total Reward: 12.0\n",
            "Episode: 10, Total Reward: 13.0\n",
            "Episode: 11, Total Reward: 16.0\n",
            "Episode: 12, Total Reward: 11.0\n",
            "Episode: 13, Total Reward: 11.0\n",
            "Episode: 14, Total Reward: 9.0\n",
            "Episode: 15, Total Reward: 83.0\n",
            "Episode: 16, Total Reward: 46.0\n",
            "Episode: 17, Total Reward: 109.0\n",
            "Episode: 18, Total Reward: 50.0\n",
            "Episode: 19, Total Reward: 68.0\n",
            "Episode: 20, Total Reward: 38.0\n",
            "Episode: 21, Total Reward: 67.0\n",
            "Episode: 22, Total Reward: 61.0\n",
            "Episode: 23, Total Reward: 41.0\n",
            "Episode: 24, Total Reward: 85.0\n",
            "Episode: 25, Total Reward: 61.0\n",
            "Episode: 26, Total Reward: 117.0\n",
            "Episode: 27, Total Reward: 45.0\n",
            "Episode: 28, Total Reward: 65.0\n",
            "Episode: 29, Total Reward: 73.0\n",
            "Episode: 30, Total Reward: 59.0\n",
            "Episode: 31, Total Reward: 28.0\n",
            "Episode: 32, Total Reward: 54.0\n",
            "Episode: 33, Total Reward: 78.0\n",
            "Episode: 34, Total Reward: 79.0\n",
            "Episode: 35, Total Reward: 69.0\n",
            "Episode: 36, Total Reward: 85.0\n",
            "Episode: 37, Total Reward: 75.0\n",
            "Episode: 38, Total Reward: 128.0\n",
            "Episode: 39, Total Reward: 59.0\n",
            "Episode: 40, Total Reward: 56.0\n",
            "Episode: 41, Total Reward: 127.0\n",
            "Episode: 42, Total Reward: 117.0\n",
            "Episode: 43, Total Reward: 47.0\n",
            "Episode: 44, Total Reward: 78.0\n",
            "Episode: 45, Total Reward: 163.0\n",
            "Episode: 46, Total Reward: 79.0\n",
            "Episode: 47, Total Reward: 74.0\n",
            "Episode: 48, Total Reward: 103.0\n",
            "Episode: 49, Total Reward: 71.0\n",
            "Episode: 50, Total Reward: 133.0\n",
            "Episode: 51, Total Reward: 205.0\n",
            "Episode: 52, Total Reward: 255.0\n",
            "Episode: 53, Total Reward: 186.0\n",
            "Episode: 54, Total Reward: 114.0\n",
            "Episode: 55, Total Reward: 464.0\n",
            "Episode: 56, Total Reward: 500.0\n",
            "Episode: 57, Total Reward: 500.0\n",
            "Episode: 58, Total Reward: 500.0\n",
            "Episode: 59, Total Reward: 500.0\n",
            "Episode: 60, Total Reward: 421.0\n",
            "Episode: 61, Total Reward: 500.0\n",
            "Episode: 62, Total Reward: 500.0\n",
            "Episode: 63, Total Reward: 500.0\n",
            "Episode: 64, Total Reward: 500.0\n",
            "Episode: 65, Total Reward: 500.0\n",
            "Episode: 66, Total Reward: 500.0\n",
            "Episode: 67, Total Reward: 500.0\n",
            "Episode: 68, Total Reward: 500.0\n",
            "Episode: 69, Total Reward: 500.0\n",
            "Episode: 70, Total Reward: 500.0\n",
            "Episode: 71, Total Reward: 500.0\n",
            "Episode: 72, Total Reward: 500.0\n",
            "Episode: 73, Total Reward: 500.0\n",
            "Episode: 74, Total Reward: 500.0\n",
            "Episode: 75, Total Reward: 500.0\n",
            "Episode: 76, Total Reward: 500.0\n",
            "Episode: 77, Total Reward: 500.0\n",
            "Episode: 78, Total Reward: 500.0\n",
            "Episode: 79, Total Reward: 500.0\n",
            "Episode: 80, Total Reward: 500.0\n",
            "Episode: 81, Total Reward: 500.0\n",
            "Episode: 82, Total Reward: 500.0\n",
            "Episode: 83, Total Reward: 500.0\n",
            "Episode: 84, Total Reward: 500.0\n",
            "Episode: 85, Total Reward: 500.0\n",
            "Episode: 86, Total Reward: 500.0\n",
            "Episode: 87, Total Reward: 500.0\n",
            "Episode: 88, Total Reward: 500.0\n",
            "Episode: 89, Total Reward: 500.0\n",
            "Episode: 90, Total Reward: 500.0\n",
            "Episode: 91, Total Reward: 500.0\n",
            "Episode: 92, Total Reward: 500.0\n",
            "Episode: 93, Total Reward: 500.0\n",
            "Episode: 94, Total Reward: 500.0\n",
            "Episode: 95, Total Reward: 500.0\n",
            "Episode: 96, Total Reward: 500.0\n",
            "Episode: 97, Total Reward: 500.0\n",
            "Episode: 98, Total Reward: 500.0\n",
            "Episode: 99, Total Reward: 417.0\n",
            "Episode: 100, Total Reward: 384.0\n",
            "Episode: 101, Total Reward: 423.0\n",
            "Episode: 102, Total Reward: 428.0\n",
            "Episode: 103, Total Reward: 376.0\n",
            "Episode: 104, Total Reward: 497.0\n",
            "Episode: 105, Total Reward: 500.0\n",
            "Episode: 106, Total Reward: 476.0\n",
            "Episode: 107, Total Reward: 500.0\n",
            "Episode: 108, Total Reward: 400.0\n",
            "Episode: 109, Total Reward: 500.0\n",
            "Episode: 110, Total Reward: 500.0\n",
            "Episode: 111, Total Reward: 500.0\n",
            "Episode: 112, Total Reward: 500.0\n",
            "Episode: 113, Total Reward: 500.0\n",
            "Episode: 114, Total Reward: 500.0\n",
            "Episode: 115, Total Reward: 500.0\n",
            "Episode: 116, Total Reward: 500.0\n",
            "Episode: 117, Total Reward: 500.0\n",
            "Episode: 118, Total Reward: 500.0\n",
            "Episode: 119, Total Reward: 500.0\n",
            "Episode: 120, Total Reward: 500.0\n",
            "Episode: 121, Total Reward: 500.0\n",
            "Episode: 122, Total Reward: 500.0\n",
            "Episode: 123, Total Reward: 500.0\n",
            "Episode: 124, Total Reward: 500.0\n",
            "Episode: 125, Total Reward: 500.0\n",
            "Episode: 126, Total Reward: 500.0\n",
            "Episode: 127, Total Reward: 500.0\n",
            "Episode: 128, Total Reward: 500.0\n",
            "Episode: 129, Total Reward: 500.0\n",
            "Episode: 130, Total Reward: 500.0\n",
            "Episode: 131, Total Reward: 500.0\n",
            "Episode: 132, Total Reward: 500.0\n",
            "Episode: 133, Total Reward: 402.0\n",
            "Episode: 134, Total Reward: 406.0\n",
            "Episode: 135, Total Reward: 427.0\n",
            "Episode: 136, Total Reward: 500.0\n",
            "Episode: 137, Total Reward: 500.0\n",
            "Episode: 138, Total Reward: 500.0\n",
            "Episode: 139, Total Reward: 500.0\n",
            "Episode: 140, Total Reward: 500.0\n",
            "Episode: 141, Total Reward: 476.0\n",
            "Episode: 142, Total Reward: 385.0\n",
            "Episode: 143, Total Reward: 500.0\n",
            "Episode: 144, Total Reward: 500.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-7267dbac3c2e>\u001b[0m in \u001b[0;36m<cell line: 151>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-7267dbac3c2e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, state, action, reward, next_state, done, batch_size)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_q_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/decorators.py\u001b[0m in \u001b[0;36mdisable\u001b[0;34m(fn, recursive)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minnermost_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDisableContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDisableContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsourcefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36mgetsourcefile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0mall_bytecode_suffixes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachinery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEBUG_BYTECODE_SUFFIXES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[0mall_bytecode_suffixes\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmachinery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZED_BYTECODE_SUFFIXES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_bytecode_suffixes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m         filename = (os.path.splitext(filename)[0] +\n\u001b[1;32m    822\u001b[0m                     importlib.machinery.SOURCE_SUFFIXES[0])\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "93hbRSeAAiUB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}